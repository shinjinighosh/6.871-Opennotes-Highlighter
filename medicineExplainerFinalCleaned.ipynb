{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "medicineExplainerFinalCleaned.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "q_6111FdZ97d",
        "outputId": "bc73110c-2b24-4e94-80d5-6193211f757e"
      },
      "source": [
        "# DOWNLOADS and IMPORTS\n",
        "!pip install anvil-uplink\n",
        "!pip install cloudscraper\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import string\n",
        "import cloudscraper\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "# ANVIL SERVER SETUP\n",
        "import anvil.server\n",
        "anvil.server.connect(\"QRYI6F3U3MUJFO4J7F27WAU7-WH56GKJFAI26ZPDJ\")\n",
        "\n",
        "# NLP library\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting anvil-uplink\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/65/776713490bfd5145ddb87834355bf7936bd233b273098e37dc12f1ac253c/anvil_uplink-0.3.36-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hCollecting ws4py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/20/4019a739b2eefe9282d3822ef6a225250af964b117356971bd55e274193c/ws4py-0.5.1.tar.gz (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (1.15.0)\n",
            "Collecting argparse\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.16.0)\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-cp37-none-any.whl size=45216 sha256=997b239edda871fd8e8545439e03b3fddf42a590d5c066231cf67535491fbbe5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/6e/4e/8b0ae12fb9b8a05715256952cf7609a8ab86285fab99b88c68\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.3.36 argparse-1.4.0 ws4py-0.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting cloudscraper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/6c/a9e697edbb8b5439d2c1f98b9d7b53e0b14ce2327dafb88fcf95dcccfeec/cloudscraper-1.2.58-py2.py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.4.7)\n",
            "Collecting requests-toolbelt>=0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (3.0.4)\n",
            "Installing collected packages: requests-toolbelt, cloudscraper\n",
            "Successfully installed cloudscraper-1.2.58 requests-toolbelt-0.9.1\n",
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default environment (dev)\" as SERVER\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNhZ6QA2bSJz",
        "outputId": "fbcd3915-c2d3-47f7-8261-9471194d8973"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "user = \"shinjini\" #anil, vibha, shinjini, stuti\n",
        "\n",
        "dat_file = 'Data_Bigger.csv'\n",
        "if user == \"vibha\": \n",
        "  filepath = '/content/drive/MyDrive/MITDocuments/6.871 MLHC Project/'\n",
        "elif user == \"shinjini\":\n",
        "  filepath = '/content/'\n",
        "else:\n",
        "  filepath = '/content/drive/MyDrive/'\n",
        "\n",
        "df = pd.read_csv(filepath + dat_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2UWXBxINoIY"
      },
      "source": [
        "def get_meds_from_row(i, with_dose=False):\n",
        "  \"\"\" Given: a row number and an optional argument whether we want the medications\n",
        "  with dosage information\n",
        "  Returns: a set of medications given to the patient in corresponding row\"\"\"\n",
        "  meds = set()\n",
        "  \n",
        "  med_starts = ['discharge medications:', 'medications on discharge:']\n",
        "  med_start_idx = -1\n",
        "  j = 0\n",
        "  while med_start_idx < 0 and j < len(med_starts):\n",
        "    med_start_idx = df.text[i].lower().find(med_starts[j])\n",
        "    if med_start_idx > 0:\n",
        "      med_start_idx = med_start_idx + len(med_starts[j])\n",
        "    j = j + 1\n",
        "  \n",
        "  med_ends = ['discharge', 'follow up', '[**']\n",
        "  med_end_idx = -1\n",
        "  j = 0\n",
        "  while med_end_idx < 0 and j < len(med_ends):\n",
        "    med_end_idx = df.text[i][med_start_idx:].lower().find(med_ends[j])\n",
        "    j = j + 1\n",
        "\n",
        "  med_text = df.text[i][med_start_idx:med_start_idx+med_end_idx]\n",
        "  meds_list_unprocessed = med_text.strip().split(\"\\n\")\n",
        "  for med_description in meds_list_unprocessed:\n",
        "    try:\n",
        "      med_name = re.search('\\d\\. (.+?) (\\d+\\.?\\d*|one|two) (m.*?(g|q)|unit|puff)', med_description).group(1)\n",
        "      meds.add(med_name)\n",
        "    except:\n",
        "      try:\n",
        "        med_name = re.search('(.+?) (\\d+\\.?\\d*|one|two) (m.*?(g|q)|unit|puff)', med_description).group(1)\n",
        "        meds.add(med_name)\n",
        "      except:\n",
        "        continue\n",
        "  return meds\n",
        "\n",
        "def get_conds_from_row(i):\n",
        "  \"\"\" Given: a row number\n",
        "  Returns: a set of patient conditions for corresponding row\"\"\"\n",
        "  conds = set()\n",
        "  \n",
        "  cond_starts = ['discharge diagnosis:', 'final diagnoses:', 'discharge diagnoses:', 'final diagnosis:']\n",
        "  cond_start_idx = -1\n",
        "  j = 0\n",
        "  while cond_start_idx < 0 and j < len(cond_starts):\n",
        "    cond_start_idx = df.text[i].lower().find(cond_starts[j])\n",
        "    if cond_start_idx > 0:\n",
        "      cond_start_idx = cond_start_idx + len(cond_starts[j])\n",
        "    j += 1\n",
        "  \n",
        "  cond_ends = ['discharge', 'follow up', '[**']\n",
        "  cond_end_idx = -1\n",
        "  j = 0\n",
        "  while cond_end_idx < 0 and j < len(cond_ends):\n",
        "    cond_end_idx = df.text[i][cond_start_idx:].lower().find(cond_ends[j])\n",
        "    j += 1\n",
        "\n",
        "  cond_text = df.text[i][cond_start_idx:cond_start_idx+cond_end_idx]\n",
        "  cond_list_unprocessed = cond_text.strip().split(\"\\n\")\n",
        "  #print(cond_list_unprocessed)\n",
        "  for cond_description in cond_list_unprocessed:\n",
        "    try:\n",
        "      cond_name = re.search('\\d\\.\\s*(.+?)(\\.|,|$)', cond_description).group(1)\n",
        "      conds.add(cond_name)\n",
        "    except:\n",
        "      try:\n",
        "        cond_name = re.search('(.+?)\\s*(\\.|,|\\[|$)', cond_description).group(1)\n",
        "        conds.add(cond_name)\n",
        "      except:\n",
        "        continue\n",
        "  return conds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5B_7xgCbEF_"
      },
      "source": [
        "def get_google_def(med):\n",
        "  \"\"\"Given: a medication name\n",
        "  Returns: drug name, brand name, description, typical and severe side effects from webmd\"\"\"\n",
        "  #print(\"Searching google/webmd\")\n",
        "  drug_name = \"\"\n",
        "  brand_names = []\n",
        "  desc = \"Not found\" \n",
        "  ts = []\n",
        "  ss = []\n",
        "  i = 0\n",
        "  url1 = \"https://www.google.com/search?q=\" + med + \"+webmd\"\n",
        "  scraper = cloudscraper.create_scraper(delay = 15)\n",
        "  my_text = scraper.get(url1).text\n",
        "  soup = BeautifulSoup(my_text, \"html.parser\")\n",
        "  \n",
        "  links = soup.find_all('a')\n",
        "  for l in links:\n",
        "    try:\n",
        "      if \"https://www.webmd.com/drugs/2\" in l.get('href'):\n",
        "        k = l.get('href')\n",
        "        myLink = k.split(\"details\")[0] + \"details\"\n",
        "        myLink = \"https://\" + k.split(\"https://\")[1]\n",
        "        break\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "  if myLink is None:\n",
        "    return drug_name, brand_names, desc, ts, ss\n",
        "  else:\n",
        "    my_text = scraper.get(myLink).text\n",
        "    soup = BeautifulSoup(my_text, \"html.parser\")\n",
        "    try:\n",
        "      desc = soup.find_all(class_=\"drug-uses\")[-1].getText()\n",
        "    except:\n",
        "      desc = soup.find(class_=\"monograph-content\").getText()\n",
        "    desc = desc.encode(\"ascii\", \"ignore\")\n",
        "\n",
        "    try: \n",
        "      drug_name = soup.find(class_=\"drug-name\").getText()\n",
        "    except:\n",
        "      drug_name = \"Not found\"\n",
        "    \n",
        "    try:\n",
        "      bnames = soup.find_all(class_=\"drug-generic-name\")\n",
        "      for b in bnames:\n",
        "        brand_names.append(b.getText())\n",
        "    except:\n",
        "      brand_names = brand_names\n",
        "    return drug_name, brand_names, desc, ts, ss\n",
        "\n",
        "\n",
        "def get_medline_description(med):\n",
        "  \"\"\"Given: a medication name\n",
        "  Returns: drug name, brand name, description, typical and severe side effects from medline\"\"\"\n",
        "  drug_name = \"\"\n",
        "  brand_names = []\n",
        "  desc = \"Not found\" \n",
        "  ts = []\n",
        "  ss = []\n",
        "  URL_1 = 'https://vsearch.nlm.nih.gov/vivisimo/cgi-bin/query-meta?v%3Aproject=medlineplus&v%3Asources=medlineplus-bundle&query=' + med + '&binning-state=group%3d%3dDrugs%20and%20Supplements&'\n",
        "  scraper = cloudscraper.create_scraper(delay = 15)  \n",
        "  my_text = scraper.get(URL_1).text\n",
        "  soup = BeautifulSoup(my_text, \"html.parser\")\n",
        "\n",
        "  #Going through the outputted urls in order, \n",
        "  #Picking the first one unless we deem it incorrect\n",
        "  matches = soup.find_all(class_ = \"url\")\n",
        "  names = soup.find_all(class_= \"title\")\n",
        "  i = 0\n",
        "  name = \"Ravioli, ravioli, \"\n",
        "  URL_2 = \"give me the formuoli\"\n",
        "  while (notCorrect(med, name, URL_2)):\n",
        "    try:\n",
        "      URL_2 = matches[i].text\n",
        "    except:\n",
        "      desc = \"Not found\"\n",
        "      return drug_name, brand_names, desc, ts, ss\n",
        "    name = names[i].text\n",
        "    i = i + 1\n",
        "  # Going to the drug's page\n",
        "  try:\n",
        "    scraper = cloudscraper.create_scraper(delay = 15)\n",
        "    my_text_2 = scraper.get(URL_2).text\n",
        "    soup2 = BeautifulSoup(my_text_2, \"html.parser\")\n",
        "    matches_2 = soup2.find_all(id = \"why\")\n",
        "    desc = matches_2[0].contents[1].text\n",
        "    desc = desc.encode(\"ascii\", \"ignore\")\n",
        "\n",
        "    #Getting the drug name\n",
        "    matches_3 = soup2.find(class_=\"with-also\")\n",
        "    drug_name = matches_3.text\n",
        "    #Getting brand names for the drug\n",
        "    try:\n",
        "      matches_4 = soup2.find(id=\"section-brandname-1\").contents[0]\n",
        "    except:\n",
        "      matches_4 = soup2.find(id=\"section-brand-name-2\").contents[0]\n",
        "    brand_names = []\n",
        "    for li in matches_4.find_all('li'):\n",
        "      brand_names.append(li.text)\n",
        "\n",
        "    side_effect_matches = soup2.find(id = \"side-effects\")\n",
        "    try:\n",
        "      typical_sides = side_effect_matches.contents[1].contents[1]\n",
        "      ts = []\n",
        "      for li in typical_sides.find_all('li'):\n",
        "        ts.append(li.text)\n",
        "    except:\n",
        "      ts = []\n",
        "    \n",
        "    try:\n",
        "      severe_sides = side_effect_matches.contents[1].contents[3]\n",
        "      ss = []\n",
        "      for li in severe_sides.find_all('li'):\n",
        "        ss.append(li.text)\n",
        "      for li in severe_sides.parent.contents[2].find_all('li'):\n",
        "        ss.append(li.text)\n",
        "    except:\n",
        "      ss = []\n",
        "    return drug_name, brand_names, desc, ts, ss\n",
        "  except:\n",
        "    desc = \"Not found\"\n",
        "  return drug_name, brand_names, desc, ts, ss\n",
        "\n",
        "@anvil.server.callable\n",
        "#Helper function for get_med_descriptions\n",
        "def get_med_description(med, get_side_effect = False):\n",
        "\n",
        "  '''\n",
        "  Search medlineplus for a description of med\n",
        "  Input: string\n",
        "  Output: string\n",
        "  '''\n",
        "  med = str(med)\n",
        "  med = med.lower()\n",
        "  med = med.replace(\" \", \"+\")\n",
        "\n",
        "  drug_name = \"\"\n",
        "  brand_names = []\n",
        "  desc = \"Not found\" \n",
        "  ts = []\n",
        "  ss = []\n",
        "\n",
        "  try:\n",
        "    drug_name, brand_names, desc, ts, ss = get_medline_description(med)\n",
        "    if desc != \"Not found\" and med in desc.lower():\n",
        "      return drug_name, brand_names, desc, ts, ss\n",
        "  except:\n",
        "    desc = \"Not found\"\n",
        "  \n",
        "  try:\n",
        "    drug_name, brand_names, desc, ts, ss= get_google_def(med)\n",
        "    if desc != \"Not found\":\n",
        "      return drug_name, brand_names, desc, ts, ss\n",
        "  except:\n",
        "    desc = \"Not found\"\n",
        "  \n",
        "  return drug_name, brand_names, desc, ts, ss\n",
        "\n",
        "\n",
        "#Helper function for selecting valid drug search URL\n",
        "#Add cases as we test\n",
        "def notCorrect(med, name, URL):\n",
        "  \"\"\"Given: a medicine, name and URL\n",
        "  Returns: if the URL is the valid drug search URL for the given medication\"\"\"\n",
        "  m = med.lower()\n",
        "  n = name.lower()\n",
        "  if name + URL == \"Ravioli, ravioli, give me the formuoli\":\n",
        "    return True\n",
        "  if \"injection\" not in m and \"injection\" in n:\n",
        "    return True\n",
        "  elif \" and \" not in m and \" and \" in n:\n",
        "    return True\n",
        "  elif \"druginfo\" not in URL:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUms2TBTeU8z"
      },
      "source": [
        "def get_google_cond(cond):\n",
        "  \"\"\"Given: a condition name\n",
        "  Returns: the webmd description for the condition \n",
        "  \"\"\"\n",
        "  desc = \"Not found\"\n",
        "  try:\n",
        "    url1 = \"https://www.google.com/search?q=\" + cond + \"+webmd\"\n",
        "    scraper = cloudscraper.create_scraper(delay = 15)\n",
        "    my_text = scraper.get(url1).text\n",
        "    soup = BeautifulSoup(my_text, \"html.parser\")\n",
        "    \n",
        "    links = soup.find_all('a')\n",
        "    for l in links:\n",
        "      try:\n",
        "        if \"https://www.webmd.com/\" in l.get('href'):\n",
        "          k = l.get('href')\n",
        "          myLink = \"https://\" + k.split(\"https://\")[1]\n",
        "          break\n",
        "      except:\n",
        "        continue\n",
        "    \n",
        "    if myLink is None:\n",
        "      return desc\n",
        "    else:\n",
        "      my_text = scraper.get(myLink).text\n",
        "      soup = BeautifulSoup(my_text, \"html.parser\")\n",
        "      try:\n",
        "        desc = soup.find(class_=\"article-page active-page\").getText()\n",
        "      except:\n",
        "        return desc\n",
        "      desc = desc.encode(\"ascii\", \"ignore\")\n",
        "    return desc\n",
        "  except:\n",
        "    return desc\n",
        "\n",
        "def get_full_desc(condition):\n",
        "  '''\n",
        "  Search medlineplus for a description of condition\n",
        "  Input: string\n",
        "  Output: string\n",
        "  '''\n",
        "  try:\n",
        "    cond = str(condition)\n",
        "    cond = cond.replace(\" \", \"+\")\n",
        "    URL_1 = 'https://vsearch.nlm.nih.gov/vivisimo/cgi-bin/query-meta?v%3Aproject=medlineplus&v%3Asources=medlineplus-bundle&query=' + cond + '&binning-state=group%3d%3dHealth%20Topics&'\n",
        "    scraper = cloudscraper.create_scraper(delay = 15)  \n",
        "    my_text = scraper.get(URL_1).text\n",
        "    soup = BeautifulSoup(my_text, \"html.parser\")\n",
        "\n",
        "    #Going through the outputted urls in order, \n",
        "    #Picking the first one unless we deem it incorrect\n",
        "    matches = soup.find_all(class_ = \"url\")\n",
        "    names = soup.find_all(class_= \"title\")\n",
        "    i = 0\n",
        "    name = \"Ravioli, ravioli, \"\n",
        "    URL_2 = \"give me the formuoli\"\n",
        "    while (notCorrectCond(cond, name, URL_2)):\n",
        "      try:\n",
        "        URL_2 = matches[i].text\n",
        "      except:\n",
        "        desc = \"Not found\"\n",
        "        return get_google_cond(cond)\n",
        "      name = names[i].text\n",
        "      i = i + 1\n",
        "\n",
        "    scraper = cloudscraper.create_scraper(delay = 15)\n",
        "    my_text_2 = scraper.get(URL_2).text\n",
        "    soup2 = BeautifulSoup(my_text_2, \"html.parser\")\n",
        "\n",
        "    #Getting the condition full description\n",
        "    matches_2 = soup2.find_all(id = \"topsum_section\")\n",
        "    matches_summary = matches_2[0]\n",
        "    desc = matches_summary.getText()\n",
        "    desc = desc.encode(\"ascii\", \"ignore\")\n",
        "    return desc\n",
        "  except:\n",
        "    desc = get_google_cond(cond)\n",
        "    return desc\n",
        "\n",
        "\n",
        "#Helper function for selecting valid drug search URL\n",
        "#Add cases as we test\n",
        "def notCorrectCond(cond, name, URL):\n",
        "  m = cond.lower()\n",
        "  n = name.lower()\n",
        "  if name + URL == \"Ravioli, ravioli, give me the formuoli\":\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loPCIoW9OF8H"
      },
      "source": [
        "# Functions to get patient conditions and medications\n",
        "@anvil.server.callable\n",
        "def get_pt_conditions(hadm_id, df=df):\n",
        "  \"\"\"\n",
        "  Input: a dataframe and a patient id\n",
        "  Returns: a list of condition names\n",
        "  \"\"\"\n",
        "  all_conds = set()\n",
        "  for index, row in df.iterrows():\n",
        "    if row.hadm_id == hadm_id:\n",
        "      all_conds.update(get_conds_from_row(index))\n",
        "  orig_cond_list = sorted(list(all_conds))\n",
        "  final_conds_list = []\n",
        "  for i, c in enumerate(orig_cond_list):\n",
        "    con = get_full_desc(c)\n",
        "    if con != \"Not found\":\n",
        "      final_conds_list.append(c)\n",
        "\n",
        "  return final_conds_list\n",
        "\n",
        "@anvil.server.callable\n",
        "def get_pt_medications(hadm_id, with_dose=False, df=df):\n",
        "  \"\"\" \n",
        "  Given: a dataframe of patient information, hadm_id and an optional argument whether we want the medications\n",
        "  with dosage information\n",
        "  Returns: a list of medications, with dose details depending on parameter\n",
        "  \"\"\"\n",
        "  all_meds = set()\n",
        "  for index, row in df.iterrows():\n",
        "    if row.hadm_id == hadm_id:\n",
        "      all_meds.update(get_meds_from_row(index, with_dose=with_dose))\n",
        "  \n",
        "  orig_med_list = sorted(list(all_meds))\n",
        "  final_med_list = []\n",
        "  for i, m in enumerate(orig_med_list):\n",
        "    d, b, mdesc, n, s = get_med_description(m)\n",
        "    if mdesc != \"Not found\":\n",
        "      final_med_list.append(m)\n",
        "  return final_med_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioev9gXUwEKZ"
      },
      "source": [
        "# Functions for NLP training step\n",
        "def get_corpus(paragraphs):\n",
        "  \"\"\"\n",
        "  Input: list of strings\n",
        "  Takes each string and strips it so that only nouns remain \n",
        "  Output: tuple, first item is a list of noun-only strings, \n",
        "    second is list of integers representing the length of the noun-only strings\n",
        "  \"\"\"\n",
        "  nltk_types = ['NN', 'NNP', 'NNS', 'NNPS']  #nltk.help.upenn_tagset() to see docum.\n",
        "  corpus = []\n",
        "  lens = []\n",
        "  for par in paragraphs:\n",
        "    tokens = nltk.word_tokenize(par)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    nouns_only = ' '.join([word for word, typ in tagged if typ in nltk_types])\n",
        "    corpus.append(nouns_only)\n",
        "    lens.append(len(nouns_only.split()))\n",
        "  lens = np.array(lens)[:, None]\n",
        "  return corpus, lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB2LQSk4TohX",
        "outputId": "fb26c834-85f4-4e9d-dce0-6a8d1dec8179"
      },
      "source": [
        "corpus_pickle_location = filepath + 'corpus.pickle'\n",
        "\n",
        "# Load re-saved corpuses\n",
        "if path.exists(corpus_pickle_location):\n",
        "  # load the saved corpuses\n",
        "  with open(corpus_pickle_location, 'rb') as handle:\n",
        "    corpus_conds, cond_lens, corpus_meds, med_lens, corpus_rands, rand_lens, corpus_conds_rand, corpus_meds_rand, vectorizer = pickle.load(handle)\n",
        "\n",
        "# total number of noun-only sentences in each corpus\n",
        "num_conds_total = len(corpus_conds)\n",
        "num_meds_total = len(corpus_meds)\n",
        "num_rands_total = len(corpus_rands)\n",
        "\n",
        "# fit the vectorizer to the corpus\n",
        "vectorizer = CountVectorizer(ngram_range= (1, 1))\n",
        "corpus_full_nums = vectorizer.fit_transform(corpus_conds + corpus_meds)\n",
        "\n",
        "# transform corpuses with vectorizer\n",
        "cnarr_conds = vectorizer.transform(corpus_conds).toarray()\n",
        "cnarr_meds = vectorizer.transform(corpus_meds).toarray()\n",
        "cnarr_rands = vectorizer.transform(corpus_rands).toarray()\n",
        "\n",
        "# percent of articles with at least 1 occurence\n",
        "word_freqs_conds = np.sum((cnarr_conds > 0).astype(int), axis = 0)/num_conds_total\n",
        "word_freqs_meds = np.sum((cnarr_meds > 0).astype(int), axis = 0)/num_meds_total\n",
        "word_freqs_rands = np.sum((cnarr_rands > 0).astype(int), axis = 0)/num_rands_total\n",
        "\n",
        "# avg times per word per article normalized by article len\n",
        "avg_word_counts_conds = np.sum(cnarr_conds[:(num_conds_total)]/cond_lens, axis = 0)/(num_conds_total)\n",
        "avg_word_counts_meds = np.sum(cnarr_meds[:(num_meds_total)]/med_lens, axis = 0)/(num_meds_total)\n",
        "avg_word_counts_rands = np.sum(cnarr_rands[:(num_rands_total)]/rand_lens, axis = 0)/(num_rands_total)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flarfoOOvW3K"
      },
      "source": [
        "def isImportantWord(i, condvec, medvec, condlen, medlen, word_list):\n",
        "  \"\"\"\n",
        "  Function to determine if a word is \"important\". \n",
        "  - it must appear infrequently across all articles\n",
        "    ie \"disease\" and \"treatment\" are too common but \"cholesterol\" and \"antidepressant\" are more unique\n",
        "  - it must be more common for the given condition+medication than the average condition+medication\n",
        "    ie \"blood pressure\" might appear once or twice in most articles, but 4 times for hypertension and hypertension drugs\n",
        "  \"\"\"\n",
        "  cond_freq = .25\n",
        "  med_freq = .25\n",
        "  rand_freq =.01 #.0015 #we are actually referring to true rand freq/cond freq\n",
        "  \n",
        "  # important word if the word appears in less than 1/nth of the cond/med articles (+ wiki descriptions)\n",
        "  if word_freqs_conds[i] < cond_freq and word_freqs_meds[i] < med_freq and word_freqs_rands[i]/word_freqs_conds[i] < rand_freq: \n",
        "    # important word if the word appears more in the chosen cond on average than it does in the average condition and likewise for med\n",
        "    if condvec[0, i]/condlen > 1.5*avg_word_counts_conds[i] and medvec[0, i]/medlen > 1.5*avg_word_counts_meds[i]:\n",
        "      #print(word_list[i] + \" \" +str(np.round(word_freqs_conds[i], 4)) + \" \" + str(np.round(word_freqs_meds[i], 4)) + \" \" + str(np.round(word_freqs_rands[i], 4)))\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def get_mutual_words(cond, med, cond_descr, med_descr):\n",
        "  \"\"\"\n",
        "  Finds (noun-only) words that are in both articles and are \"important\" (see helper function). \n",
        "  Selects the words that have highest frequency and outputs them as the keywords\n",
        "  \"\"\"\n",
        "  # preprocess in the same way we did to the corpus\n",
        "  cond_descr, clen = get_corpus([cond_descr])\n",
        "  med_descr, mlen = get_corpus([med_descr])\n",
        "  # cond_descr, med_descr, clen, mlen all are lists of len 1\n",
        "  \n",
        "  cond_vec = vectorizer.transform(cond_descr)\n",
        "  med_vec = vectorizer.transform(med_descr)\n",
        "  word_list = vectorizer.get_feature_names()\n",
        "  \n",
        "  # how many of each word in the paragraph (1d vectors)\n",
        "  cond_word_count = cond_vec.toarray().sum(axis=0)\n",
        "  med_word_count = med_vec.toarray().sum(axis=0)\n",
        "\n",
        "  words_in_both_idx = np.logical_and(cond_word_count > 0, med_word_count > 0)\n",
        "  both_idx = np.nonzero(words_in_both_idx)[0]\n",
        "  mutual_words = []\n",
        "  keywords = []\n",
        "  for i in both_idx:\n",
        "    if isImportantWord(i, cond_vec, med_vec, clen, mlen, word_list):\n",
        "      mutual_words.append(word_list[i])\n",
        "\n",
        "  if mutual_words:\n",
        "    max_freq = sorted(mutual_words, key = lambda i: i[1], reverse = True)[0][1]\n",
        "    keywords = [word for word in mutual_words if word[1]==max_freq]\n",
        "  if (\" \" + cond.lower() + \" \") in med_descr[0] and cond.lower() not in mutual_words:\n",
        "    mutual_words.append(cond.lower())\n",
        "    keywords.append(cond.lower())\n",
        "\n",
        "  return keywords, mutual_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8-lUFZgS0rP"
      },
      "source": [
        "@anvil.server.callable\n",
        "def get_med_info(med, all_conditions, patient_id=190539, df=df):\n",
        "  \n",
        "  conditions = all_conditions\n",
        "  drug_names, brand_names, med_descr, norm_sides, severe_sides = get_med_description(med, True)\n",
        "\n",
        "  try:\n",
        "    med_descr = med_descr.decode('UTF-8')\n",
        "  except:\n",
        "    med_descr = med_descr\n",
        "\n",
        "  key_conditions = dict()\n",
        "  for cond in conditions:\n",
        "    cond_descr = get_full_desc(cond)\n",
        "    try:\n",
        "      cond_descr = cond_descr.decode('UTF-8')\n",
        "    except:\n",
        "      cond_descr = cond_descr\n",
        "    keywords, mutual_words = get_mutual_words(cond, med, str(cond_descr), str(med_descr))\n",
        "    if mutual_words:\n",
        "      key_conditions[cond] = mutual_words\n",
        "      \n",
        "  return med,med_descr,drug_names,brand_names,norm_sides,severe_sides, key_conditions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NBUb4wJmnGK"
      },
      "source": [
        "# Anvil functionality/user flow\n",
        "\n",
        "# 1) enter patient id (hadm_id)?\n",
        "#     -> get_pt_medications() and display all medications\n",
        "#     -> get_pt_conditions() and display all conditions\n",
        "#     Note: each med should be a clickable button that can change color\n",
        "#     Note: each condition should be text that can be highlighted on an event\n",
        "\n",
        "# 2) select radio medication button\n",
        "#     -> get_med_info() to get MedInfo object for the med\n",
        "#     -> populate the cells with descriptions, side effects, etc\n",
        "#     -> choose which conditions to highlight and change the formatting\n",
        "#     -> REFRESH/EXPLAIN button click to get global\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa7udq1cutj6"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcSGqJvgesI0"
      },
      "source": [
        "def test_patient_medications(df):\n",
        "  all_ids = df.hadm_id\n",
        "  total = len(all_ids)\n",
        "  count = 0\n",
        "  for id in tqdm(all_ids):\n",
        "    res = get_pt_medications(id, df)\n",
        "    print(id, \":\", res)\n",
        "    if res:\n",
        "      count += 1\n",
        "  print(count, \" out of \", total, \" found\")\n",
        "    \n",
        "test_patient_medications(df[:150])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLRdZrBX6G2y"
      },
      "source": [
        "def test_patient_conditions(df):\n",
        "  all_ids = df.hadm_id.unique()\n",
        "  total = len(all_ids)\n",
        "  count = 0\n",
        "  for id in tqdm(all_ids):\n",
        "    res = get_pt_conditions(id, df)\n",
        "    print(id, \":\", res)\n",
        "    if res:\n",
        "      count += 1\n",
        "  print(count, \" out of \", total, \" found\")\n",
        "\n",
        "test_patient_conditions(df[:150])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}