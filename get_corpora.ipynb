{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"get_corpora.ipynb","provenance":[{"file_id":"1hJNWhEei8wo2aQMA8OOBAX4kN3baMbe5","timestamp":1620273696971}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"_fIqtZCoOaHY"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaE4DpoYeJPJ","executionInfo":{"status":"ok","timestamp":1620946383168,"user_tz":240,"elapsed":7039,"user":{"displayName":"Anil Palepu","photoUrl":"","userId":"08657457796097655401"}},"outputId":"c82d5a76-90c5-4980-8c65-ef112cda33bf"},"source":["#CONDITIONS\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","!pip install cloudscraper\n","import cloudscraper\n","import string\n","from tqdm import tqdm\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('tagsets')\n","\n","\n","\n","# dummy_words = ['that','as', 'an','and','on','in','for','the','or','of','to','its','a','other','this','is','about','it','can','these','may','cause','with','also','called','because','order','when','why','who','at','alone','such','does','with','without','are','your','into', 'make', 'making', 'from', 'so','are','time','exercise','diet',            \"too\",\"much\",\"between\",\"one\",\"two\",\"weight\",\"treated\",\"alcohol\",\"days\",\"if\",\"over\",\"way\", ]\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting cloudscraper\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/6c/a9e697edbb8b5439d2c1f98b9d7b53e0b14ce2327dafb88fcf95dcccfeec/cloudscraper-1.2.58-py2.py3-none-any.whl (96kB)\n","\r\u001b[K     |███▍                            | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20kB 19.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30kB 23.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40kB 26.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51kB 30.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 71kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 81kB 30.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92kB 31.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 9.5MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.4.7)\n","Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.23.0)\n","Collecting requests-toolbelt>=0.9.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (3.0.4)\n","Installing collected packages: requests-toolbelt, cloudscraper\n","Successfully installed cloudscraper-1.2.58 requests-toolbelt-0.9.1\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Unzipping help/tagsets.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"NRftJvymOmIY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620946498232,"user_tz":240,"elapsed":64602,"user":{"displayName":"Anil Palepu","photoUrl":"","userId":"08657457796097655401"}},"outputId":"dc024f95-56ee-4bf2-feda-5c66addffe5e"},"source":["def getAllConditions():\n","  \"\"\"\n","  webcraping function to get paragraphs for many conditions from medlineplus\n","  \"\"\"\n","  locs = [\"https://medlineplus.gov/foodandnutrition.html\", \"https://medlineplus.gov/bloodheartandcirculation.html\", \"https://medlineplus.gov/bonesjointsandmuscles.html\", \"https://medlineplus.gov/brainandnerves.html\", \"https://medlineplus.gov/digestivesystem.html\", \"https://medlineplus.gov/earnoseandthroat.html\", \"https://medlineplus.gov/endocrinesystem.html\", \"https://medlineplus.gov/eyesandvision.html\", \"https://medlineplus.gov/immunesystem.html\", \"https://medlineplus.gov/kidneysandurinarysystem.html\", \"https://medlineplus.gov/lungsandbreathing.html\", \"https://medlineplus.gov/mouthandteeth.html\", \"https://medlineplus.gov/femalereproductivesystem.html\", \"https://medlineplus.gov/skinhairandnails.html\", \"https://medlineplus.gov/malereproductivesystem.html\"]\n","  all_conds = []\n","  for l in locs:\n","    scraper = cloudscraper.create_scraper(delay = 15)  \n","    my_text = scraper.get(l).text\n","    soup = BeautifulSoup(my_text, \"html.parser\")\n","    matches = soup.find(id = 'tpgp')\n","    links = [a.get('href') for a in matches.find_all('a', href=True)]\n","    link2 = []\n","    cter = 0;\n","    for l2 in links:\n","      if '.html' in l2:\n","        link2.append(l2)\n","        cter = cter + 1\n","    all_conds = all_conds + link2\n","\n","  all_unique_conds = np.unique(np.array(all_conds))\n","\n","  cond_desc = []\n","  for cond in all_unique_conds:\n","    scraper = cloudscraper.create_scraper(delay = 15)  \n","    my_text = scraper.get(cond).text\n","    soup = BeautifulSoup(my_text, \"html.parser\")\n","    matches_2 = soup.find_all(id = \"topsum_section\")\n","    matches_summary = matches_2[0]\n","    desc = matches_summary.getText()\n","    desc = desc.lower()\n","    desc = desc.replace('\\n', ' ')\n","    cond_desc.append(desc)\n","  return np.array(cond_desc)\n","\n","paragraphs_conds = getAllConditions()\n","print(paragraphs_conds.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(575,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VA8EuyE8ZIZ","executionInfo":{"status":"ok","timestamp":1620759404428,"user_tz":240,"elapsed":365351,"user":{"displayName":"Anil Palepu","photoUrl":"","userId":"08657457796097655401"}},"outputId":"46d55a27-231a-48d3-eb4a-f7032f07e4a6"},"source":["def getAllMeds():\n","  \"\"\"\n","  webcraping function to get paragraphs for many medications from medlineplus\n","  \"\"\"\n","  locs = []\n","  for s in string.ascii_uppercase:\n","    locs.append('https://medlineplus.gov/druginfo/drug_' + s + 'a.html')\n","  \n","  all_meds = []\n","  for l in locs:\n","    scraper = cloudscraper.create_scraper(delay = 15)  \n","    my_text = scraper.get(l).text\n","    soup = BeautifulSoup(my_text, \"html.parser\")\n","    matches = soup.find(id = 'index')\n","    links = [a.get('href') for a in matches.find_all('a', href=True)]\n","    link2 = []\n","    for l2 in links:\n","      if '.html' in l2:\n","        link2.append(l2)\n","    all_meds = all_meds + link2\n","\n","  all_unique_meds = np.unique(np.array(all_meds))\n","  print(all_unique_meds.shape)\n","\n","  med_desc = []\n","  for med in all_unique_meds:\n","    try:\n","      myUrl = 'https://medlineplus.gov/druginfo' + med[1:]\n","      scraper = cloudscraper.create_scraper(delay = 15)  \n","      my_text = scraper.get(myUrl).text\n","      soup = BeautifulSoup(my_text, \"html.parser\")\n","      matches_2 = soup.find_all(id = \"why\")\n","      desc = matches_2[0].contents[1].text\n","      desc = desc.lower()\n","      desc = desc.replace('\\n', ' ')\n","      med_desc.append(desc)\n","    except:\n","      print(\"error\")\n","      continue\n","  return np.array(med_desc)\n","\n","paragraphs_meds = getAllMeds()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1604,)\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n","error\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"44wYl01ziZwy"},"source":["def getAllRandom(numRands = 5000, max_words_per_page = 150):\n","  \"\"\"\n","  webcraping function to get max_words_per_page words from numRands articles \n","  from wikipedia\n","  \"\"\"\n","  locs = []\n","  for s in range(numRands):\n","    locs.append('https://en.wikipedia.org/wiki/Special:Random')\n","  print(f\"fetched all {numRands} articles\")\n","\n","  rand_desc = []\n","  for l in tqdm(locs):\n","    scraper = cloudscraper.create_scraper(delay = 15)  \n","    my_text = scraper.get(l).text\n","    soup = BeautifulSoup(my_text, \"html.parser\")\n","    desc = soup.find(class_='mw-parser-output')\n","    descs = desc.find_all('p')\n","    desc = \"\"\n","    for d in descs:\n","      desc = desc + d.getText()\n","\n","    desc = ' '.join(desc.split()[:max_words_per_page])\n","    desc = desc.lower()\n","    desc = desc.replace('\\n', ' ')\n","    rand_desc.append(desc)\n","\n","  return rand_desc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oqlb9hp0PjoC"},"source":["def get_corpus(paragraphs):\n","  \"\"\"\n","  Input: list of strings\n","  Takes each string and strips it so that only nouns remain \n","  Output: tuple, first item is a list of noun-only strings, \n","    second is list of integers representing the length of the noun-only strings\n","  \"\"\"\n","  nltk_types = ['NN', 'NNP', 'NNS', 'NNPS']  #nltk.help.upenn_tagset() to see docum.\n","  corpus = []\n","  lens = []\n","  for par in paragraphs:\n","    tokens = nltk.word_tokenize(par)\n","    tagged = nltk.pos_tag(tokens)\n","    nouns_only = ' '.join([word for word, typ in tagged if typ in nltk_types])\n","    corpus.append(nouns_only)\n","    lens.append(len(nouns_only.split()))\n","  lens = np.array(lens)[:, None]\n","  return corpus, lens\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89C8r7eTAVml","executionInfo":{"status":"ok","timestamp":1620762165685,"user_tz":240,"elapsed":490962,"user":{"displayName":"Anil Palepu","photoUrl":"","userId":"08657457796097655401"}},"outputId":"b2d6fb7f-654c-4812-c5f5-c4cd1d54f58e"},"source":["corpus_conds, cond_lens = get_corpus(paragraphs_conds)\n","corpus_meds, med_lens = get_corpus(paragraphs_meds)\n","paragraphs_rands = getAllRandom()\n","corpus_rands, rand_lens = get_corpus(paragraphs_rands)\n","corpus_conds_rand = corpus_conds + corpus_rands\n","corpus_meds_rand = corpus_meds + corpus_rands\n","corpus_full = corpus_conds + corpus_meds + corpus_rands\n","vectorizer = CountVectorizer(ngram_range= (1, 1))\n","cfn = vectorizer.fit_transform(corpus_full)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/5000 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["getting random corpus\n","fetched all 5000 articles\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 5000/5000 [45:07<00:00,  1.85it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTMA0q22JGaY","executionInfo":{"status":"ok","timestamp":1620762176462,"user_tz":240,"elapsed":904,"user":{"displayName":"Anil Palepu","photoUrl":"","userId":"08657457796097655401"}},"outputId":"42c495c5-92ac-49be-9b16-746b11eb2372"},"source":["import pickle\n","import os.path\n","from os import path\n","from google.colab import drive\n","\n","get_random_corpus = True\n","include_random_corpus = True\n","\n","drive.mount('/content/drive')\n","if path.exists('/content/drive/MyDrive/corpus.pickle'):\n","  with open('/content/drive/MyDrive/corpus.pickle', 'rb') as handle:\n","    corpus_conds, cond_lens, corpus_meds, med_lens, corpus_rands, rand_lens, corpus_conds_rand, corpus_meds_rand, vectorizer = pickle.load(handle)\n","\n","else:\n","  with open('/content/drive/MyDrive/corpus.pickle', 'wb') as handle:\n","    pickle.dump([corpus_conds, cond_lens, corpus_meds, med_lens, corpus_rands, rand_lens, corpus_conds_rand, corpus_meds_rand, vectorizer], handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]}]}